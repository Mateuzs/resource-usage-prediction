{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'resource-usage-data' # S3 bucket for training and model data\n",
    "prefix = 'sagemaker/resource-usage-prediction' \n",
    " \n",
    "# Define IAM role\n",
    "import sagemaker\n",
    "\n",
    "import sagemaker.predictor\n",
    "from sagemaker.predictor import csv_serializer, json_deserializer\n",
    "import boto3\n",
    "import  s3fs\n",
    "import re\n",
    "from sagemaker import get_execution_role\n",
    "import json\n",
    "import math\n",
    "from os import path\n",
    "import sagemaker.amazon.common as smac\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import glob\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "smclient = boto3.Session().client('sagemaker')\n",
    "\n",
    "s3_data_path = \"{}/{}/data\".format(bucket, prefix)\n",
    "s3_output_path = \"{}/{}/output\".format(bucket, prefix)\n",
    "\n",
    "# configure container image to be used for the region we are running in.\n",
    "# Should be the same as the region of our S3 bucket.\n",
    "containers = {\n",
    "    'us-east-1': '522234722520.dkr.ecr.us-east-1.amazonaws.com/forecasting-deepar:latest',\n",
    "}\n",
    "image_name = containers[boto3.Session().region_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the data and extract .zip file\n",
    "!wget http://gwa.ewi.tudelft.nl/fileadmin/pds/trace-archives/grid-workloads-archive/datasets/gwa-t-12/rnd.zip\n",
    "\n",
    "import zipfile\n",
    "with zipfile.ZipFile(\"rnd.zip\",\"r\") as zip_ref: zip_ref.extractall(\"targetdir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into one data frame\n",
    "files = glob.glob(os.path.join('targetdir/rnd/2013-7', \"*.csv\"))\n",
    "files_first200 = files[:300]\n",
    "dfs = [pd.read_csv(fp, sep = ';\\t', engine='python').assign(VM=os.path.basename(fp).split('.')[0]) for fp in files_first200]\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "files2 = glob.glob(os.path.join('targetdir/rnd/2013-8', \"*.csv\"))\n",
    "files2_first200 = files2[:300]\n",
    "dfs2 = [pd.read_csv(fp, sep = ';\\t', engine='python').assign(VM=os.path.basename(fp).split('.')[0]) for fp in files2_first200]\n",
    "df2 = pd.concat(dfs2, ignore_index=True)\n",
    "\n",
    "files3 = glob.glob(os.path.join('targetdir/rnd/2013-9', \"*.csv\"))\n",
    "files3_first200 = files3[:300]\n",
    "dfs3 = [pd.read_csv(fp, sep = ';\\t', engine='python').assign(VM=os.path.basename(fp).split('.')[0]) for fp in files3_first200]\n",
    "df3 = pd.concat(dfs3, ignore_index=True)\n",
    "\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = df.append(df2)\n",
    "data2 = data1.append(df3)\n",
    "data_frame = data2\n",
    "data_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formatting\n",
    "data_frame['Timestamp'] = pd.to_datetime(data_frame['Timestamp [ms]'], unit = 's')\n",
    "data_frame.describe()\n",
    "data_frame['weekday'] = data_frame['Timestamp'].dt.dayofweek\n",
    "data_frame['weekend'] = ((data_frame.weekday) // 5 == 1).astype(float)\n",
    "\n",
    "# Feature engineering with the date\n",
    "data_frame['month']=data_frame.Timestamp.dt.month \n",
    "data_frame['day']=data_frame.Timestamp.dt.day\n",
    "data_frame.set_index('Timestamp',inplace=True)\n",
    "data_frame[\"CPU usage prev\"] = data_frame['CPU usage [%]'].shift(1)\n",
    "data_frame[\"CPU_diff\"] = data_frame['CPU usage [%]'] - data_frame[\"CPU usage prev\"]\n",
    "data_frame[\"received_prev\"] = data_frame['Network received throughput [KB/s]'].shift(1)\n",
    "data_frame[\"received_diff\"] = data_frame['Network received throughput [KB/s]']- data_frame[\"received_prev\"]\n",
    "data_frame[\"transmitted_prev\"] = data_frame['Network transmitted throughput [KB/s]'].shift(1)\n",
    "data_frame[\"transmitted_diff\"] = data_frame['Network transmitted throughput [KB/s]']- data_frame[\"transmitted_prev\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame[\"start\"] = data_frame.index\n",
    "data_frame['target'] = data_frame['CPU usage [MHZ]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = data_frame.groupby('VM').resample('1min', how={'target':np.mean})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = data_frame.groupby('VM').resample('1min', how={'CPU capacity provisioned [MHZ]':np.mean})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.reset_index(level=0, inplace=True)\n",
    "df3 = df3.fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.reset_index(level=0, inplace=True)\n",
    "df2 = df2.fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#format data into json\n",
    "\n",
    "freq = \"1min\"\n",
    "context_length = 30\n",
    "prediction_length = 30\n",
    "\n",
    "def series_to_obj(ts, cat=None):\n",
    "    obj = {\"start\": str(ts.index[0]), \"target\": list(ts)}\n",
    "    if cat is not None:\n",
    "        obj[\"cat\"] = cat\n",
    "    return obj\n",
    "\n",
    "def series_to_jsonline(ts, cat=None):\n",
    "    return json.dumps(series_to_obj(ts, cat))\n",
    "\n",
    "\n",
    "\n",
    "time_series_test=[]\n",
    "vm_index_range = df2['VM'].unique()\n",
    "for i in vm_index_range:\n",
    "    newseries = df2[df2['VM'] == i]['target']\n",
    "    del newseries.index.name\n",
    "    newseries.index = pd.to_datetime(newseries.index)\n",
    "    time_series_test.append(newseries)\n",
    "    \n",
    "    \n",
    "time_series_training=[]\n",
    "vm_index_range = df2['VM'].unique()\n",
    "for i in vm_index_range:\n",
    "    newseries = df2[df2['VM'] == i]['target']\n",
    "    del newseries.index.name\n",
    "    newseries.index = pd.to_datetime(newseries.index)\n",
    "    time_series_training.append(newseries[:-prediction_length])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# push the json data to S# bucket\n",
    "\n",
    "s3filesystem = s3fs.S3FileSystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This functions converts the test and train data into JSON lines for Sagemaker\n",
    "\n",
    "encoding = \"utf-8\"\n",
    "\n",
    "with s3filesystem.open(s3_data_path + \"/test/test_data.json\", 'wb') as fp:\n",
    "    for ts in time_series_test:\n",
    "        fp.write(series_to_jsonline(ts).encode(encoding))\n",
    "        fp.write('\\n'.encode(encoding))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with s3filesystem.open(s3_data_path + \"/train/train_data.json\", 'wb') as fp:\n",
    "    for ts in time_series_training:\n",
    "        fp.write(series_to_jsonline(ts).encode(encoding))\n",
    "        fp.write('\\n'.encode(encoding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set hyperparameters\n",
    "\n",
    "estimator = sagemaker.estimator.Estimator(\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    image_name=image_name,\n",
    "    role=role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.c4.xlarge',\n",
    "    base_job_name='test-demo-deepar',\n",
    "    output_path=\"s3://\" + s3_output_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters  = {\n",
    "    \"time_freq\": freq,\n",
    "    \"context_length\": context_length,\n",
    "    \"prediction_length\": prediction_length,\n",
    "    \"num_cells\": \"32\",\n",
    "    \"num_layers\": \"2\",\n",
    "    \"likelihood\": \"student-t\",\n",
    "    \"epochs\": \"20\",\n",
    "    \"mini_batch_size\": \"32\",\n",
    "    \"learning_rate\": \"0.001\",\n",
    "    \"dropout_rate\": \"0.05\",\n",
    "    \"early_stopping_patience\": \"10\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next line tells SageMaker to start an EC2 instance, \n",
    "# download the data from S3, start training the model and save the trained model.\n",
    "\n",
    "estimator.set_hyperparameters(**hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "\n",
    "data_channels = {\n",
    "    \"train\": \"s3://{}/train/\".format(s3_data_path),\n",
    "    \"test\": \"s3://{}/test/\".format(s3_data_path)\n",
    "}\n",
    "\n",
    "estimator.fit(inputs=data_channels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create endpoint and predictor\n",
    "job_name = estimator.latest_training_job.name\n",
    "\n",
    "endpoint_name = sagemaker_session.endpoint_from_job(\n",
    "    job_name=job_name,\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    "    deployment_image=image_name,\n",
    "    role=role\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepARPredictor(sagemaker.predictor.RealTimePredictor):\n",
    "\n",
    "    def set_prediction_parameters(self, freq, prediction_length):\n",
    "        self.freq = freq\n",
    "        self.prediction_length = prediction_length\n",
    "        \n",
    "    def predict(self, ts, cat=None, encoding=\"utf-8\", num_samples=100, quantiles=[\"0.1\", \"0.5\", \"0.9\"]):\n",
    "        prediction_times = [x.index[-1]+1 for x in ts]\n",
    "        req = self.__encode_request(ts, cat, encoding, num_samples, quantiles)\n",
    "        res = super(DeepARPredictor, self).predict(req)\n",
    "        return self.__decode_response(res, prediction_times, encoding)\n",
    "    \n",
    "    def __encode_request(self, ts, cat, encoding, num_samples, quantiles):\n",
    "        instances = [series_to_obj(ts[k], cat[k] if cat else None) for k in range(len(ts))]\n",
    "        configuration = {\"num_samples\": num_samples, \"output_types\": [\"quantiles\"], \"quantiles\": quantiles}\n",
    "        http_request_data = {\"instances\": instances, \"configuration\": configuration}\n",
    "        return json.dumps(http_request_data).encode(encoding)\n",
    "    \n",
    "    def __decode_response(self, response, prediction_times, encoding):\n",
    "        response_data = json.loads(response.decode(encoding))\n",
    "        list_of_df = []\n",
    "        for k in range(len(prediction_times)):\n",
    "            prediction_index = pd.DatetimeIndex(start=prediction_times[k], freq=self.freq, periods=self.prediction_length)\n",
    "            list_of_df.append(pd.DataFrame(data=response_data['predictions'][k]['quantiles'], index=prediction_index))\n",
    "        return list_of_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = DeepARPredictor(\n",
    "    endpoint=endpoint_name,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    content_type=\"application/json\"\n",
    ")\n",
    "predictor.set_prediction_parameters(freq, prediction_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize Model Predictions\n",
    "\n",
    "new_time_series_training = []\n",
    "for ts in time_series_training:\n",
    "    new_time_series_training.append(ts.asfreq('T'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_time_series_test = []\n",
    "for ts in time_series_test:\n",
    "    new_time_series_test.append(ts.asfreq('T'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_df  = predictor.predict(new_time_series_training[1:2]) # predicted forecast\n",
    "actual_data = new_time_series_test[1:2] # full data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(len(list_of_df)): \n",
    "    plt.style.use('seaborn-white')\n",
    "    plt.figure(figsize=(12,6))\n",
    "    actual_data[k][-prediction_length-context_length:].plot(label='Actual',linewidth = 2.5)\n",
    "    p10 = list_of_df[k]['0.1'] \n",
    "    p90 = list_of_df[k]['0.9'] #set limits predictively\n",
    "    plt.fill_between(p10.index, p10, p90, alpha=0.5, label='80% Confidence Interval')\n",
    "    list_of_df[k]['0.5'].plot(label='Prediction Median', color = 'orange',linewidth = 2.5) # set requests for capacity allocation \n",
    "    plt.title(\"DeepAR Model Prediction\", fontsize = 23)\n",
    "    plt.ylabel(\"CPU usage [MHz]\", fontsize = 20)\n",
    "    #plt.yticks([10,20.40,50])\n",
    "    plt.xlabel(\"Time\", fontsize = 20)\n",
    "    (list_of_df[k]['0.9']+100).plot(label='My Suggested Provision', color = 'g',linewidth = 2.5) # set requests for capacity allocation \n",
    "    plt.yticks(fontsize=14);\n",
    "    #plt.axhline(y=5851.99912, color='r', linestyle='-', label = 'Actual Provision')\n",
    "    plt.xticks(fontsize=14);\n",
    "    plt.legend(fontsize = 12,loc = 'best')\n",
    "    #plt.savefig('VM101-withactual')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
